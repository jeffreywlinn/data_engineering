# data_engineering

Below is a brief description of the three projects listed above in the repository.
#
ETL PROJECT 3 (Python, Apache Spark, Databricks, AWS Services, SQL):

Python notebooks used in Apache Spark and AWS Lambda for cleaning batch and streaming weather data streams into an AWS S3 bucket. Data was processed through bronze, silver, and gold layers to AWS Redshift for further analysis and AWS Quicksight for data visualization.
#
ETL PROJECT 2 (Python, Visual Studio, SQL, Apache Spark, YARN, Databricks, AWS S3 Storage):

Python notebooks cleaning GitHub event archive data. Scalable code preserving as much of the original information as possible. Targeted insights and aggregations for event types, messages, and language detection.
#
ETL PROJECT 1 (Python, Visual Studio, PostGreSQL, AWS S3 Storage:

Python notebooks for cleaning and writing to csv files. Cleaned US Customer and Border Patrol shipping data for a use case. Created data lake with bronze, silver and gold layers. Postgres PGAdmin4 for the data warehouse. Data partitioned by year. 
#


